{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import threading\n",
        "import glob\n",
        "import sys\n",
        "\n",
        "# --- 1. Configuración ---\n",
        "try:\n",
        "    import pyspark\n",
        "except ImportError:\n",
        "    print(\"Instalando PySpark...\")\n",
        "    !pip install pyspark\n",
        "    !apt-get install openjdk-17-jdk -qq > /dev/null\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"PYSPARK_PYTHON\"] = \"python\"\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\"\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "# --- 2. Limpieza Total ---\n",
        "try:\n",
        "    from pyspark.streaming import StreamingContext\n",
        "    active_ssc = StreamingContext.getActive()\n",
        "    if active_ssc is not None:\n",
        "        active_ssc.stop(stopSparkContext=True, stopGracefully=False)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    SparkContext.getOrCreate().stop()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# --- 3. Preparación de Directorios ---\n",
        "BASE_DIR = \"/content\"\n",
        "SOURCE_DIR = os.path.join(BASE_DIR, \"origen_datos\")\n",
        "INPUT_DIR = os.path.join(BASE_DIR, \"datos_streaming\")\n",
        "CHECKPOINT_DIR = \"./checkpoint_tp3\"\n",
        "\n",
        "if os.path.exists(CHECKPOINT_DIR): shutil.rmtree(CHECKPOINT_DIR)\n",
        "if os.path.exists(INPUT_DIR): shutil.rmtree(INPUT_DIR)\n",
        "os.makedirs(INPUT_DIR)\n",
        "\n",
        "if not os.path.exists(SOURCE_DIR):\n",
        "    os.makedirs(SOURCE_DIR)\n",
        "    print(f\"Se creó el directorio {SOURCE_DIR}\")\n",
        "    print(\">>> Subí los archivos ahí y volvé a ejecutar.\")\n",
        "\n",
        "# --- 4. Lógica Spark Streaming ---\n",
        "\n",
        "BATCH_INTERVAL = 5\n",
        "A = 1.0\n",
        "\n",
        "sc = SparkContext(\"local[*]\", \"TP3_Final_Fix\")\n",
        "ssc = StreamingContext(sc, BATCH_INTERVAL)\n",
        "ssc.checkpoint(CHECKPOINT_DIR)\n",
        "\n",
        "# Hacemos que Spark mire la carpeta\n",
        "stream = ssc.textFileStream(INPUT_DIR)\n",
        "\n",
        "def parsear_linea(linea):\n",
        "    try:\n",
        "        parts = linea.split(\"\\t\")\n",
        "        if len(parts) >= 3:\n",
        "            return (int(parts[1]), float(parts[2]))\n",
        "        return None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def actualizar_state(nuevos_valores, estado_anterior):\n",
        "    score = estado_anterior if estado_anterior is not None else 0.0\n",
        "    promedio_ventana = sum(nuevos_valores) if nuevos_valores else 0.0\n",
        "\n",
        "    # Se descuenta A en todas las ventanas, haya combate o no\n",
        "\n",
        "    nuevo_score = score + promedio_ventana - A\n",
        "    return nuevo_score\n",
        "\n",
        "# Pipeline\n",
        "parsed = stream.map(parsear_linea).filter(lambda x: x is not None)\n",
        "\n",
        "# Promedio local por ventana\n",
        "promedios_ventana = parsed.mapValues(lambda x: (x, 1))\\\n",
        "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\\\n",
        "    .mapValues(lambda v: v[0] / v[1])\n",
        "\n",
        "# Actualización Global\n",
        "scores_globales = promedios_ventana.updateStateByKey(actualizar_state)\n",
        "\n",
        "# Ordenar Descendente\n",
        "scores_ordenados = scores_globales.transform(\n",
        "    lambda rdd: rdd.sortBy(lambda x: x[1], ascending=False)\n",
        ")\n",
        "\n",
        "# Mostrar\n",
        "scores_ordenados.pprint(num=100)\n",
        "\n",
        "# --- 5. INYECTOR ---\n",
        "# El inyector busca simular la llegada de datos de manera progresiva, copia uno a uno los archivos del dataset hacia el directorio de procesamiento\n",
        "def mover_archivos_uno_a_uno():\n",
        "    time.sleep(5)\n",
        "\n",
        "\n",
        "    while(True):\n",
        "      with os.scandir(SOURCE_DIR) as it:\n",
        "        for i, archivo in enumerate(it):\n",
        "          if archivo.is_file():\n",
        "            nombre = os.path.basename(archivo)\n",
        "            destino = os.path.join(INPUT_DIR, nombre)\n",
        "\n",
        "            # 1. Copia del archivo\n",
        "            shutil.move(archivo, destino)\n",
        "\n",
        "            # 2. Se actualiza la fecha de última modificación de los archivos, de esta manera Spark cree que son nuevos (la fecha debe ser posterior a la ejecución)\"\n",
        "            os.utime(destino, None)\n",
        "\n",
        "            print(f\">>> [MOVIDO] Archivo '{nombre}' ingresado.\")\n",
        "\n",
        "            time.sleep(4)\n",
        "\n",
        "hilo = threading.Thread(target=mover_archivos_uno_a_uno)\n",
        "hilo.start()\n",
        "\n",
        "# --- 6. Ejecución ---\n",
        "print(\">>> Spark Streaming iniciado...\")\n",
        "ssc.start()\n",
        "\n",
        "# Dejar descomentada una sola forma:\n",
        "\n",
        "#ssc.awaitTermination() # Ejecución infinita\n",
        "\n",
        "try:\n",
        "    time.sleep(300) # Cuánto tiempo durará la ejecución\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "finally:\n",
        "    ssc.stop(stopSparkContext=True)\n",
        "    print(\">>> Fin.\")"
      ],
      "metadata": {
        "id": "yrQL_ETC51_c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}